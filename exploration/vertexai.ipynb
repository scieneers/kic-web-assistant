{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitral 7B on Vertex AI with [vLLM](https://github.com/vllm-project/vllm) \n",
    "Following [this documentation](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/148?project=kic-chat-assistant) and [this notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb)\n",
    "\n",
    "## Notes\n",
    "Vertex AI currently offers no cold starts, updates here[](https://issuetracker.google.com/issues/206042974?pli=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"kic-chat-assistant\"\n",
    "REGION = \"europe-west4\"\n",
    "SERVICE_ACCOUNT = \"vertexai-endpoint-sa@kic-chat-assistant.iam.gserviceaccount.com\"\n",
    "# For experiment outputs\n",
    "BUCKET_URI = \"gs://vertexai_mistral\"\n",
    "STAGING_BUCKET = f\"{BUCKET_URI}/temporal\"\n",
    "# The pre-built serving docker image with vLLM\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(model_name, model_id, service_account, machine_type=\"g2-standard-8\", accelerator_type=\"NVIDIA_L4\", accelerator_count=1):\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    dtype = \"bfloat16\"\n",
    "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
    "        dtype = \"float16\"\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272/operations/9083172484563337216\n",
      "Endpoint created. Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/675164168178/locations/europe-west4/endpoints/5312809399088054272')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/675164168178/locations/europe-west4/models/8617395994415333376/operations/2789392005313069056\n",
      "Model created. Resource name: projects/675164168178/locations/europe-west4/models/8617395994415333376@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/675164168178/locations/europe-west4/models/8617395994415333376@1')\n",
      "Deploying model to Endpoint : projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "Deploy Endpoint model backing LRO: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272/operations/7401078023740456960\n",
      "Endpoint model deployed. Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n"
     ]
    }
   ],
   "source": [
    "prebuilt_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Find Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "# Pricing: https://cloud.google.com/vertex-ai/pricing#pred_eur\n",
    "\n",
    "# Proposed configurations and pricing per hour for europe-west4 region:\n",
    "# n1-standard-16 with 2 T4 GPUs    : $1.0123 + 2* GPU $0.4370 \n",
    "# n1-standard-16 with 2 V100 GPUs  : $1.0123 + 2* GPU $2.9325\n",
    "# g2-standard-8 with 1 L4 GPU      : $1.081  + GPU included?\n",
    "# a2-highgpu-1g with 1 A100 GPU    : $4.3103 + GPU included!\n",
    "\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"mistral-serve-vllm\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming:\n",
      "Prompt:\n",
      "My favourite condiment is\n",
      "Output:\n",
      " Garlic butter. It adds a beautiful flavour to any dish. I love using garlic butter to roast garlic or potatoes, to make garlic bread, or to bake chicken. Garlic butter is a versatile ingredient that adds depth of flavor to so many dishes. If you love garlic butter as much as I do, you need to try this recipe.\n",
      "\n",
      "Garlic Butter Recipe\n",
      "\n",
      "Ingredients:\n",
      "1 cup unsalted butter (225 grams)\n",
      "4 garlic cloves, peeled\n",
      "1/4 teaspoon salt\n",
      "1/4 teaspoon black pepper\n",
      "1 tablespoon freshly chopped parsley or chives\n",
      "\n",
      "Instructions:\n",
      "1. Melt the butter in a small saucepan over medium-low heat or in a pot in a double boiler. Make sure the butter does not boil, just melt.\n",
      "2. Once the butter has melted, add the garlic cloves to the\n",
      "Output:\n",
      " Prompt:\n",
      "My favourite condiment is\n",
      "Output:\n",
      " Garlic butter. It adds a beautiful flavour to any dish. I love using garlic butter to roast garlic or potatoes, to make garlic bread, or to bake chicken. Garlic butter is a versatile ingredient that adds depth of flavor to so many dishes. If you love garlic butter as much as I do, you need to try this recipe.\n",
      "\n",
      "Garlic Butter Recipe\n",
      "\n",
      "Ingredients:\n",
      "1 cup unsalted butter (225 grams)\n",
      "4 garlic cloves, peeled\n",
      "1/4 teaspoon salt\n",
      "1/4 teaspoon black pepper\n",
      "1 tablespoon freshly chopped parsley or chives\n",
      "\n",
      "Instructions:\n",
      "1. Melt the butter in a small saucepan over medium-low heat or in a pot in a double boiler. Make sure the butter does not boil, just melt.\n",
      "2. Once the butter has melted, add the garlic cloves to the\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "instance = {\n",
    "    \"prompt\": \"My favourite condiment is\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 200,\n",
    "}\n",
    "\n",
    "def get_streaming_response(response: requests.Response):\n",
    "    for chunk in response.iter_lines(\n",
    "        chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"\n",
    "    ):\n",
    "        if chunk:\n",
    "            data = json.loads(chunk.decode(\"utf-8\"))\n",
    "            yield data\n",
    "\n",
    "response = endpoint.raw_predict(\n",
    "    body = json.dumps({\"instances\": [instance]}),\n",
    "    headers = {\"Content-Type\": \"application/json\"},\n",
    ")\n",
    " \n",
    "text_len = 0\n",
    "print(\"Streaming:\")\n",
    "for output in get_streaming_response(response):\n",
    "    text = output[\"predictions\"][0]\n",
    "    print(text[text_len:])\n",
    "    text_len = len(text)\n",
    "print(\"Output:\\n\", text)\n",
    "\n",
    "# async def get_predictions(endpoint, instance):\n",
    "#     \"\"\"Gets predictions from the deployed model.\"\"\"\n",
    "#     responses = await endpoint.serverStreamingPredict(instances=[instance])\n",
    "#     for response in responses[0]:\n",
    "#         print(response)\n",
    "\n",
    "# import asyncio\n",
    "# task = asyncio.create_task(get_predictions(endpoint, instance))  \n",
    "# await task\n",
    "# Many instances test\n",
    "# tasks = [asyncio.create_task(get_predictions(endpoint, instance)) for _ in range(10)]\n",
    "# await asyncio.gather(*tasks)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying all deployed models and deleting endpoint 5312809399088054272 [mistral-serve-vllm_20231129_154113-endpoint]\n",
      "Undeploying Endpoint model: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "Undeploy Endpoint model backing LRO: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272/operations/2102030112185647104\n",
      "Endpoint model undeployed. Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "Deleting Endpoint : projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "Delete Endpoint  backing LRO: projects/675164168178/locations/europe-west4/operations/7724774246957711360\n",
      "Endpoint deleted. . Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n"
     ]
    }
   ],
   "source": [
    "delete_endpoint = True\n",
    "def list_endpoints():\n",
    "    return [(r.name, r.display_name) for r in aiplatform.Endpoint.list() if r.display_name.startswith(\"mistral-serve-vllm\")]\n",
    "\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoints = list_endpoints()\n",
    "        for endpoint_id, endpoint_name in endpoints:\n",
    "            endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "            print(\n",
    "                f\"Undeploying all deployed models and deleting endpoint {endpoint_id} [{endpoint_name}]\"\n",
    "            )\n",
    "            endpoint.delete(force=True)\n",
    "\n",
    "        # Delete the bucket\n",
    "        #!gsutil -m rm -r $BUCKET_URI\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
