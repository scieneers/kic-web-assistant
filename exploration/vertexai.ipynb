{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitral 7B on Vertex AI with [vLLM](https://github.com/vllm-project/vllm) \n",
    "Following [this documentation](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/148?project=kic-chat-assistant) and [this notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"kic-chat-assistant\"\n",
    "REGION = \"europe-west4\"\n",
    "SERVICE_ACCOUNT = \"vertexai-endpoint-sa@kic-chat-assistant.iam.gserviceaccount.com\"\n",
    "# For experiment outputs\n",
    "BUCKET_URI = \"gs://vertexai_mistral\"\n",
    "STAGING_BUCKET = f\"{BUCKET_URI}/temporal\"\n",
    "# The pre-built serving docker image with vLLM\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(model_name, model_id, service_account, machine_type=\"g2-standard-8\", accelerator_type=\"NVIDIA_L4\", accelerator_count=1):\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    dtype = \"bfloat16\"\n",
    "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
    "        dtype = \"float16\"\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272/operations/9083172484563337216\n",
      "Endpoint created. Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/675164168178/locations/europe-west4/endpoints/5312809399088054272')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/675164168178/locations/europe-west4/models/8617395994415333376/operations/2789392005313069056\n",
      "Model created. Resource name: projects/675164168178/locations/europe-west4/models/8617395994415333376@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/675164168178/locations/europe-west4/models/8617395994415333376@1')\n",
      "Deploying model to Endpoint : projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "Deploy Endpoint model backing LRO: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272/operations/7401078023740456960\n",
      "Endpoint model deployed. Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n"
     ]
    }
   ],
   "source": [
    "prebuilt_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Find Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "# Pricing: https://cloud.google.com/vertex-ai/pricing#pred_eur\n",
    "\n",
    "# Proposed configurations and pricing per hour for europe-west4 region:\n",
    "# n1-standard-16 with 2 T4 GPUs    : $1.0123 + 2* GPU $0.4370 \n",
    "# n1-standard-16 with 2 V100 GPUs  : $1.0123 + 2* GPU $2.9325\n",
    "# g2-standard-8 with 1 L4 GPU      : $1.081  + GPU included?\n",
    "# a2-highgpu-1g with 1 A100 GPU    : $4.3103 + GPU included!\n",
    "\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"mistral-serve-vllm\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "My favourite joke goes like this:\n",
      "Output:\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "So you see, scientists are just like us, always questioning the world around us and trying to make sense of it. And that's what I love about science - it's an ongoing exploration of the unknown.\n",
      "\n",
      "So, if you're ever curious about something, don't be afraid to ask a question or seek out some answers. Science is here to help!\n",
      "Prompt:\n",
      "My favourite condiment is\n",
      "Output:\n",
      " easily sriracha sauce. It satisfies my cravings for a spicy kick and adds extra depth to my meals. Iâ€™ve even been known to incorporate it in my baking! Recently, I came across this recipe for vegan sriracha tofu that looked too good to pass up.\n",
      "\n",
      "### Ingredients:\n",
      "\n",
      "- 1 block of firm tofu (14 oz)\n",
      "- 1/4 cup of nutritional yeast\n",
      "- 2 tablespoons of sriracha sauce\n",
      "- 2 tablespoons of soy sauce\n",
      "- 1 tablespoon of rice vinegar\n",
      "- 1 clove of garlic, minced\n",
      "- 1 teaspoon of sesame oil\n",
      "- 1 teaspoon of cornstarch\n",
      "- 1/4 teaspoon of ground black pepper\n",
      "- 1/4 teaspoon of smoked paprika\n",
      "- 2 tablespoons of chopped green onions\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "instance = {\n",
    "    \"prompt\": \"My favourite condiment is\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 200,\n",
    "}\n",
    "\n",
    "async def get_predictions(endpoint, instance):\n",
    "    \"\"\"Gets predictions from the deployed model.\"\"\"\n",
    "    responses = await endpoint.predict_async(instances=[instance])\n",
    "    for response in responses[0]:\n",
    "        print(response)\n",
    "\n",
    "import asyncio\n",
    "# task = asyncio.create_task(get_predictions(endpoint, instance))  \n",
    "# await task\n",
    "# Many instances test\n",
    "tasks = [asyncio.create_task(get_predictions(endpoint, instance)) for _ in range(10)]\n",
    "await asyncio.gather(*tasks)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_endpoint = False\n",
    "def list_endpoints():\n",
    "    return [\n",
    "        (r.name, r.display_name)\n",
    "        for r in aiplatform.Endpoint.list()\n",
    "        if r.display_name.startswith(\"mistral-serve-vllm\")\n",
    "    ]\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoints = list_endpoints()\n",
    "        for endpoint_id, endpoint_name in endpoints:\n",
    "            endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "            print(\n",
    "                f\"Undeploying all deployed models and deleting endpoint {endpoint_id} [{endpoint_name}]\"\n",
    "            )\n",
    "            endpoint.delete(force=True)\n",
    "\n",
    "        # Delete the bucket\n",
    "        !gsutil -m rm -r $BUCKET_URI\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
