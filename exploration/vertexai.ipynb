{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitral 7B on Vertex AI with [vLLM](https://github.com/vllm-project/vllm) \n",
    "Following [this documentation](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/148?project=kic-chat-assistant) and [this notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_mistral.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"kic-chat-assistant\"\n",
    "REGION = \"europe-west4\"\n",
    "SERVICE_ACCOUNT = \"vertexai-endpoint-sa@kic-chat-assistant.iam.gserviceaccount.com\"\n",
    "# For experiment outputs\n",
    "BUCKET_URI = \"gs://vertexai_mistral\"\n",
    "STAGING_BUCKET = f\"{BUCKET_URI}/temporal\"\n",
    "# The pre-built serving docker image with vLLM\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str):\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model_vllm(model_name, model_id, service_account, machine_type=\"g2-standard-8\", accelerator_type=\"NVIDIA_L4\", accelerator_count=1):\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    dtype = \"bfloat16\"\n",
    "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
    "        dtype = \"float16\"\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--gpu-memory-utilization=0.9\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272/operations/9083172484563337216\n",
      "Endpoint created. Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/675164168178/locations/europe-west4/endpoints/5312809399088054272')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/675164168178/locations/europe-west4/models/8617395994415333376/operations/2789392005313069056\n",
      "Model created. Resource name: projects/675164168178/locations/europe-west4/models/8617395994415333376@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/675164168178/locations/europe-west4/models/8617395994415333376@1')\n",
      "Deploying model to Endpoint : projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n",
      "Deploy Endpoint model backing LRO: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272/operations/7401078023740456960\n",
      "Endpoint model deployed. Resource name: projects/675164168178/locations/europe-west4/endpoints/5312809399088054272\n"
     ]
    }
   ],
   "source": [
    "prebuilt_model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Find Vertex AI prediction supported accelerators and regions in\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
    "# Pricing: https://cloud.google.com/vertex-ai/pricing#pred_eur\n",
    "\n",
    "# Proposed configurations and pricing per hour for europe-west4 region:\n",
    "# n1-standard-16 with 2 T4 GPUs    : $1.0123 + 2* GPU $0.4370 \n",
    "# n1-standard-16 with 2 V100 GPUs  : $1.0123 + 2* GPU $2.9325\n",
    "# g2-standard-8 with 1 L4 GPU      : $1.081  + GPU included?\n",
    "# a2-highgpu-1g with 1 A100 GPU    : $4.3103 + GPU included!\n",
    "\n",
    "machine_type = \"g2-standard-8\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count = 1\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"mistral-serve-vllm\"),\n",
    "    model_id=prebuilt_model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "My favourite condiment is\n",
      "Output:\n",
      " curry paste. It has so many flavours and can be made with so many different ingredients. There are many different types, such as red curry paste and green curry paste. There are also many varieties within each colour.\n",
      "\n",
      "One of my favourite curry paste brands is Maekrua. They have many different types of curry pastes, including Thai basil, tom kha, massaman, and green curry. They also offer many different varieties within each type of curry paste.\n",
      "\n",
      "Another great brand is Thai Kitchen. They have a wide range of curry sauces, but their curry pastes are also very good. They offer many different types of curry pastes, including green curry paste and Thai basil curry paste.\n",
      "\n",
      "If you want to make your own curry paste, there are many recipes available online. Some popular ingredients used in curry paste include garlic, galangal, le\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "instance = {\n",
    "    \"prompt\": \"My favourite condiment is\",\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": 200,\n",
    "}\n",
    "\n",
    "async def get_predictions(endpoint, instance):\n",
    "    \"\"\"Gets predictions from the deployed model.\"\"\"\n",
    "    responses = await endpoint.predict_async(instances=[instance])\n",
    "    for response in responses[0]:\n",
    "        print(response)\n",
    "\n",
    "import asyncio\n",
    "task = asyncio.create_task(get_predictions(endpoint, instance))  \n",
    "await task\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up the endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_endpoint = False\n",
    "def list_endpoints():\n",
    "    return [\n",
    "        (r.name, r.display_name)\n",
    "        for r in aiplatform.Endpoint.list()\n",
    "        if r.display_name.startswith(\"mistral-serve-vllm\")\n",
    "    ]\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoints = list_endpoints()\n",
    "        for endpoint_id, endpoint_name in endpoints:\n",
    "            endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "            print(\n",
    "                f\"Undeploying all deployed models and deleting endpoint {endpoint_id} [{endpoint_name}]\"\n",
    "            )\n",
    "            endpoint.delete(force=True)\n",
    "\n",
    "        # Delete the bucket\n",
    "        !gsutil -m rm -r $BUCKET_URI\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
