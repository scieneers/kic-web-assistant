{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboards\n",
    "https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n",
    "https://llm-leaderboard.streamlit.app/\n",
    "https://opening-up-chatgpt.github.io/\n",
    "https://www.linkedin.com/search/results/content/?keywords=rag%20zephyr&searchId=63f84b12-44fc-403e-971b-7b0f14c12dc6&sid=aP%40&update=urn%3Ali%3Afs_updateV2%3A(urn%3Ali%3Aactivity%3A7132380266773250049%2CBLENDED_SEARCH_FEED%2CEMPTY%2CDEFAULT%2Cfalse)\n",
    "\n",
    "# Llama2 \n",
    "https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "## Not really open source:\n",
    "Can't be used to train other models: \n",
    "“v. You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof).”\n",
    "Commercial restrictions:\n",
    "“2. Additional Commercial Terms. If, on the Llama 2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee’s affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta,“\n",
    "Not approved by the [Open Source Initiative](\"https://opensource.org/licenses/\")\n",
    "\n",
    "## BUT: \n",
    "huggingface leaderboard is dominated by llama 2 models\n",
    "Open Technology to some degree\n",
    "\n",
    "# Inactive/Discontinued: \n",
    "Open Assistant\n",
    "BlOOMChat\n",
    "Pythia-Chat_base\n",
    "\n",
    "# Mistral 7B\n",
    "Nach eigener Aussage: \"No, it is free to use but not open source. The datasets and weights are proprietary.\"\n",
    "*Aber* Apache 2.0 license -> Mehr Open Source \n",
    "Zephyr finetuned und bessere Performance, aber auch als Mistral-7B-Instruct?; https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\n",
    "\n",
    "\n",
    "# Conclustions\n",
    "We will be swapping models more regularly with open source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.float32, device_map=\"auto\") \n",
    "# torch_dtype=torch.bfloat16; BFloat16 is not supported on MPS\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "# model_inputs = encodeds.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "generated_ids = model.generate(encodeds, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
